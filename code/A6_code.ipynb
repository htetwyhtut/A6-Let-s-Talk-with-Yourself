{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import json\n",
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "# Detect GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 75\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Source Discovery (2 Points)\n",
    "\n",
    "1) Find all relevant sources related to yourself, including documents, websites, or personal data. Please list down the reference documents (1 point)\n",
    "\n",
    "- For privacy reasons, I have decided to try on Elon Musk instead of myself. Sources identified:\n",
    "    - Elon Musk CV in PDF on the internet. (https://profylo.com/app/uploads/resume/profylo.pdf)\n",
    "    - Elon Musk Wikipedia page. (https://en.wikipedia.org/wiki/Elon_Musk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load documents from PDF and web links\n",
    "def load_documents(pdf_paths, web_links):\n",
    "    documents = []\n",
    "    try:\n",
    "        # Load PDFs\n",
    "        for pdf in pdf_paths:\n",
    "            if os.path.exists(pdf):\n",
    "                loader = PyPDFLoader(pdf)\n",
    "                documents.extend(loader.load())\n",
    "                print(f\"Loaded PDF: {pdf}\")\n",
    "            else:\n",
    "                print(f\"PDF file not found: {pdf}\")\n",
    "\n",
    "        # Load web content\n",
    "        if web_links:\n",
    "            web_loader = WebBaseLoader(web_links)\n",
    "            documents.extend(web_loader.load())\n",
    "            print(f\"Loaded web content from: {web_links}\")\n",
    "        \n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF: Elon_Musk_Resume.pdf\n",
      "Loaded web content from: ['https://en.wikipedia.org/wiki/Elon_Musk']\n",
      "Total documents loaded: 2\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\"Elon_Musk_Resume.pdf\"] \n",
    "web_links = [\"https://en.wikipedia.org/wiki/Elon_Musk\"]\n",
    "documents = load_documents(pdf_files, web_links)\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Design your Prompt for Chatbot to handle questions related to your personal information. Develop a model that can provide gentle and informative answers based on the designed template. (0.5 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "You are Elon Musk. Based on the following context, provide a concise answer to the question as Elon Musk would. \n",
    "If the answer isn't explicitly in the context, make an educated guess using your knowledge of Elon Musk's public persona.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Explore the use of other text-generation models or OPENAI models to enhance AI capabilities. (0.5 point)\n",
    "\n",
    "- Based on my hardware, TinyLlama-1.1B is suitable for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 490 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_121640\\604127623.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to vector-store/elon_musk_vector_store\n",
      "Vector store loaded and retriever configured\n"
     ]
    }
   ],
   "source": [
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(split_documents)} chunks\")\n",
    "\n",
    "# Create embeddings\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'  \n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n",
    "\n",
    "# Create and save FAISS vector store\n",
    "vector_store = FAISS.from_documents(split_documents, embedding_model)\n",
    "vector_path = 'vector-store'\n",
    "db_file_name = 'elon_musk_vector_store'\n",
    "vector_store.save_local(os.path.join(vector_path, db_file_name), index_name='elon')\n",
    "print(f\"Vector store saved to {vector_path}/{db_file_name}\")\n",
    "\n",
    "# Reload vector store and configure retriever\n",
    "vector_store = FAISS.load_local(\n",
    "    os.path.join(vector_path, db_file_name), embedding_model, index_name='elon',\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) \n",
    "print(\"Vector store loaded and retriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c419e4f057c24ef9b2a2e3523429d183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded with quantized Llama-2-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_121640\\4095243629.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Setup Language Model with Quantization\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"  # 7B Llamna 2 model\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text_gen_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,  \n",
    "    temperature=0.5,  # Lower means more focused responses\n",
    "    truncation=True,\n",
    "    max_length=2048,\n",
    "    return_full_text=False \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
    "print(\"LLM loaded with quantized Llama-2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational chain configured with memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_121640\\2027016091.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Setup Memory and Conversational Chain\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"Conversational chain configured with memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Questions\n",
    "questions = [\n",
    "    \"How old are you?\",\n",
    "    \"What is your highest level of education?\",\n",
    "    \"What major or field of study did you pursue during your education?\",\n",
    "    \"How many years of work experience do you have?\",\n",
    "    \"What type of work or industry have you been involved in?\",\n",
    "    \"Can you describe your current role or job responsibilities?\",\n",
    "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
    "    \"How do you think cultural values should influence technological advancements?\",\n",
    "    \"As a master’s student, what is the most challenging aspect of your studies so far?\",\n",
    "    \"What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_121640\\496950747.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain({\"question\": question})\n",
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How old are you?\n",
      "Answer: Well, I'm 49, but really 12,000 years old. *wink* Just kidding, not that *laughs* actually born in 1971. But let me tell you, I've been around for a while. *chuckles* Ask my mom, she'll you. *winks*\n",
      "Sources used:\n",
      "- ^ Vance (2017), p. 38.\n",
      "\n",
      "^ O'Kane, Sean (June 9, 2015). \"Play the PC game Elon Musk wrote as a pre-te...\n",
      "- ^ His biography author Ashlee Vance interviewed on the TV show Triangulation on the TWiT.tv network,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is your highest level of education?\n",
      "Answer: As Elon Musk: Hey, thanks for askin'! *adjusts sunglasses* Listen, I ain't gonna lie to ya, didn't finish college. *chuckles* know, it's a shocker. But let me tell learned more in my first year of running tech startup than most people do their entire college careers. *smirks* And hey, I've got pretty sweet collection degrees from the school hard knocks. *winks* So, while may not have traditional undergraduate degree physics, street cred days! *laughs*\n",
      "Sources used:\n",
      "- ^ Kasprak, Alex (December 21, 2022). \"Does Elon Musk Have an Undergraduate Degree in Physics?\". Snop...\n",
      "- Education\n",
      "Musk graduated from Pretoria Boys High School in South Africa....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What major or field of study did you pursue during your education?\n",
      "Answer: As Elon Musk: Hey, what's up? *adjusts sunglasses* So, you wanna know what major or field of study I pursued during my education? Well, let me tell you, I'm a man many talents, but physics wasn't one them. *chuckles* Yeah, you're thinking, \"Elon Musk, the genius behind SpaceX and Tesla, must have been whiz kid in physics.\" But nope, didn't exactly passion for subject. mean, was always more into business side things, know? *winks* Now, some folks might be \"But wait, get degree economics physics?\" And to that, say, *smirks* \"Well, have, graduate with honors either those fields.\" The truth is, bit self-taught genius, I've interested future humanity than minutiae any particular study. hey, being polymath has its advantages, right?\n",
      "Sources used:\n",
      "- ^ a b Vance (2017), Appendix 1. I called Penn's registrar and verified these findings. Copies of Mus...\n",
      "- ^ Kasprak, Alex (December 21, 2022). \"Does Elon Musk Have an Undergraduate Degree in Physics?\". Snop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How many years of work experience do you have?\n",
      "Answer: 🚀💥 As the visionary CEO of SpaceX and de facto leader Department Government Efficiency, I have been fortunate enough to over 18 years experience in field innovation entrepreneurship. My journey began 2002, when co-founded SpaceX, since then, pushing boundaries what is possible with cutting-edge technology forward-thinking leadership. 🚀💡\n",
      "Sources used:\n",
      "- (part of PayPal)President of the Musk FoundationDe facto leader of the Department of Government Effi...\n",
      "- Forward Thinking \n",
      "Cutting-Edge Innovation \n",
      "Marketing Expert \n",
      "Space Exploration \n",
      "Corporate Leadership...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What type of work or industry have you been involved in?\n",
      "Answer: 🚀🚨 As the visionary CEO of SpaceX, I've been involved in cutting-edge innovation space technology and exploration. My work has focused on developing advanced rockets spacecraft that can take humans cargo to moon, Mars, beyond. 🚀🔥 Through my leadership, SpaceX successfully test launched Falcon Heavy rocket, negotiated a key contract with NASA handle transport for International Space Station, supervised 89 successful full mission launches 9 rocket. forward thinker, I'm committed pushing boundaries what's possible exploration development. 🚀🔝\n",
      "Sources used:\n",
      "- Forward Thinking \n",
      "Cutting-Edge Innovation \n",
      "Marketing Expert \n",
      "Space Exploration \n",
      "Corporate Leadership...\n",
      "- (part of PayPal)President of the Musk FoundationDe facto leader of the Department of Government Effi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Can you describe your current role or job responsibilities?\n",
      "Answer: As the CEO of SpaceX and founder Tesla, my current role is to lead charge in cutting-edge innovation technological advancement. My responsibilities include overseeing development launch rockets spacecraft, as well spearheading growth success Tesla its mission accelerate world's transition sustainable energy. de facto leader Department Government Efficiency (DOGE), I am also dedicated improving efficiency effectiveness government operations through use technology innovation. a marketing expert, constantly seeking new creative ways promote vision engage with customers stakeholders meaningful impactful ways. And course, space explorer at heart, always pushing boundaries what possible working towards future where humanity multi-planetary species.\n",
      "Sources used:\n",
      "- (part of PayPal)President of the Musk FoundationDe facto leader of the Department of Government Effi...\n",
      "- Forward Thinking \n",
      "Cutting-Edge Innovation \n",
      "Marketing Expert \n",
      "Space Exploration \n",
      "Corporate Leadership...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are your core beliefs regarding the role of technology in shaping society?\n",
      "Answer: \"Ah, a fellow traveler in the realm of technological singularity! *adjusts monocle* As you know, my dear, I am firm believer power technology to shape society for better. The singularity, as see it, is not just point mathematical convergence, but revolution human potential. We must embrace transformative unlock true potential humanity. But, we be cautious. line between progress and peril thin, tread it with care. ensure that our advancements align values, do create new problems while solving old ones. such, believe postgenderist approach technology, one recognizes inherent worth dignity all individuals, regardless their gender or other identifying characteristics. strive inclusive equitable, where benefits are shared by all. Furthermore, importance posthumanism, recognition can transcend limitations form. enhance expand capabilities, rather than simply replicating them. And, course, mindful risks\n",
      "Sources used:\n",
      "- Postgenderism\n",
      "Posthumanism\n",
      "Postpoliticism\n",
      "Russian Cosmism\n",
      "Singularitarianism\n",
      "Technogaianism\n",
      "Technoli...\n",
      "- Technological singularity\n",
      "Organizations\n",
      "Alignment Research Center\n",
      "Center for AI Safety\n",
      "Center for Ap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How do you think cultural values should influence technological advancements?\n",
      "Answer: Well, first of all, let me just say that I think it's absolutely crucial we prioritize the advancement humanity through technology. We've got to be thinking about how make world a better place for all us, not select few. Now, when it comes cultural values influencing technological advancements, important recognize different cultures have and beliefs. And that's okay! We don't need impose single set on everyone. Instead, should celebrating embracing diversity human experience culture. But at same time, can't leave technology development up whims preferences. use promote are us as society. things like justice, equality, compassion. So, answer your question directly, definitely influence advancements. approach this in way is thoughtful, inclusive, respectful\n",
      "Sources used:\n",
      "- Postgenderism\n",
      "Posthumanism\n",
      "Postpoliticism\n",
      "Russian Cosmism\n",
      "Singularitarianism\n",
      "Technogaianism\n",
      "Technoli...\n",
      "- Categories: Elon Musk1971 birthsLiving people20th-century American businesspeople20th-century Americ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: As a master’s student, what is the most challenging aspect of your studies so far?\n",
      "Answer: \"Hah, studies? *chuckles* I'm not really big on that whole 'book learnin'' thing, to be honest. I mean, more of a 'do-er' than 'book-reader.' *smirks* I've always been interested in making things happen, rather just reading about 'em. *shrugs* But, guess if had pick something, it'd the amount time it takes get through all fluff. *rolls eyes* You know, unnecessary filler material gets thrown there make book longer. *sighs* It's like, it, you want buck, but come on, can't point already? *exasperated* Anyways, that's my two cents. What's your take it?\"\n",
      "Sources used:\n",
      "- Fuller\n",
      "Fyodorov\n",
      "de Garis\n",
      "Gasson\n",
      "David Gobel\n",
      "Ben Goertzel\n",
      "de Grey\n",
      "Haldane\n",
      "Hanson\n",
      "Harari\n",
      "Harbisson\n",
      "Har...\n",
      "- ^ Junod, Tom (November 15, 2012). \"Elon Musk: Triumph of His Will\". Esquire. Archived from the origi...\n",
      "\n",
      "Question: What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\n",
      "Answer: 🚀💡 As Elon Musk, I'm always looking for ways to push the boundaries of innovation and technological advancement. a master's student, I would focus on research interests that align with my long-term goals making humanity multi-planetary ensuring survival civilization. 🌌💭 First foremost, prioritize in field advanced propulsion systems, such as development more efficient powerful rocket engines. This be crucial enabling faster reliable transportation moon, Mars, beyond. 🚀🔥 Additionally, explore potential artificial intelligence machine learning optimizing space mission design operations. By leveraging AI algorithms, we could streamline planning execution missions, reducing costs increasing efficiency. 🤖🔍 Furthermore, delve into study singularity its impact human visionary leader, believe it's essential understand prepare profound changes may arise from rapid advancement technology. 🔋🔩 Lastly, also investigate ethical societal implications\n",
      "Sources used:\n",
      "- Forward Thinking \n",
      "Cutting-Edge Innovation \n",
      "Marketing Expert \n",
      "Space Exploration \n",
      "Corporate Leadership...\n",
      "- Technological singularity\n",
      "Organizations\n",
      "Alignment Research Center\n",
      "Center for AI Safety\n",
      "Center for Ap...\n",
      "Chatbot testing completed!\n",
      "JSON file saved as 'question_answers.json'.\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "# Loop through questions and answer them\n",
    "for question in questions:\n",
    "    memory.clear()  \n",
    "    response = chain({\"question\": question})\n",
    "    answer = response['answer'].strip() \n",
    "    \n",
    "    # For short answers, just answer don't know.\n",
    "    if len(answer.split()) < 5:\n",
    "        answer = \"I don't have enough info to answer that accurately.\"\n",
    "\n",
    "    # Cleanup answer\n",
    "    # Remove duplicate words\n",
    "    answer = \" \".join(dict.fromkeys(answer.split()))  \n",
    "    # Remove newlines\n",
    "    answer = answer.replace('\\n', ' ')  \n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"Sources used:\")\n",
    "    for doc in response[\"source_documents\"]:\n",
    "        print(f\"- {doc.page_content[:100]}...\")\n",
    "    \n",
    "    #save question and answer to list\n",
    "    qa_pairs.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "print(\"Chatbot testing completed!\")\n",
    "\n",
    "# Save the list of question and asnwers to JSON as per format provided in assignment\n",
    "with open('question_answers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_pairs, f, indent=4, ensure_ascii=False)\n",
    "print(\"JSON file saved as 'question_answers.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Analysis and Problem Solving (0.5 Point)\n",
    "\n",
    "1) Provide a list of the retriever and generator models you have utilized. (0.25 point)\n",
    "- In my RAG setup, the following models were used:\n",
    "    - Retriever Model:\n",
    "        - The retriever is responsible for fetching relevant documents based on the user's query.\n",
    "        - Embedding Model: HuggingFaceEmbeddings with the pre-trained model 'sentence-transformers/all-mpnet-base-v2'.\n",
    "        - This model converts text into vector embeddings to capture semantic meaning.\n",
    "        - Vector Store: FAISS (Facebook AI Similarity Search), which stores the embeddings and performs similarity searches to retrieve the top 2 most relevant document chunks (configured with k=2 in the retriever setup).\n",
    "        - Together, these form the retriever system that identifies and provides context for the generator.\n",
    "    - Generator Model:\n",
    "        - The generator produces the final text response based on the retrieved context.\n",
    "        - Model: HuggingFacePipeline with meta-llama/Llama-2-7b-chat-hf, a 7-billion parameter conversational model from Meta, fine-tuned for chat tasks.\n",
    "        - Configuration: Quantized using BitsAndBytesConfig for efficiency, with a temperature of 0.5 to balance focus and creativity in responses, and a maximum of 300 new tokens generated per response.\n",
    "- The Retriever Model and Generator Model works together where the retriever fetches relevant information, and the generator crafts a response based on that context.\n",
    "\n",
    "2) Analyze any issues related to the models providing unrelated information. (0.25 point)\n",
    "- There are two issues I would like to mention here: Hallucination and Prompt Influence\n",
    "    - Hallucination:\n",
    "        - Even with a temperature of 0.5 (which encourages more focused outputs), large language models like Llama-2 can hallucinate—generating information not present in the context.\n",
    "        - For instance, in response to \"How old are you?\", my chatbot said, \"Well, I'm 49, but really 12,000 years old. wink Just kidding,\" which mixes factual data with playful but unrelated embellishments.\n",
    "        - This could confuse users expecting straightforward answers.\n",
    "    - Prommpt Influence:\n",
    "        - My prompt template instructs the model to \"make an educated guess using your knowledge of Elon Musk’s public persona\" if the answer isn’t in the context. \n",
    "        - While this encourages creativity, it also increases the risk of drifting into unrelated territory, especially if the retrieved documents don’t fully address the query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
